# LLM Chatbot Project â€” Learning & Delivery Roadmap

This living roadmap combines the original learning journey with the current
GitHub-Projects release cadence. Each version milestone is tracked as a
GitHub Milestone and reflected on the project board.

---

## Learning Phases & Goals

### Phase 1 â€” Basic LLM-Powered Chatbot  *(v0.1 â†’ v0.4.x)*  âœ… *Under way*

**Stack**

* Python 3.10+  |  Hugging Face Transformers
* Gradio UI
* (optional) FAISS / Chroma for RAG prototyping

**Core Goals**

| Status | Item |
|----|------|
| âœ… | Load & run pre-trained models (FLAN-T5, Mistral-7B) |
| âœ… | Gradio chat UI with multi-turn history |
| âœ… | Externalise prompts, specialised prompt matching |
| âœ… | Add safety guard-rails (strict / moderate / relaxed) |
| ðŸŸ¡ | **Stretch** â€“ add RAG file-based Q&A |
| ðŸŸ¡ | **Stretch** â€“ containerise with Podman / Docker |

---

### Phase 2 â€” Fine-Tuning Your Own Model  *(planned: v0.6.x â†’ v0.7.x)*

* Hugging Face Datasets + LoRA / QLoRA
* Run locally or on OpenShift AI
* Weights & Biases for experiment tracking

Goals â†’ build a small niche dataset, fine-tune TinyLLaMA or Mistral-7B, and
compare against the base model.

---

### Phase 3 â€” Packaging, Scaling, Integration  *(v0.8.x â†’ v1.0.0)*

* Podman / OpenShift deployment
* Persistent memory back-end (Redis / SQLite / Vector DB)
* API or Slack / Telegram bot integration
* Observability (token counts, latency metrics)

---

## Delivery Milestones by Version

| Version | Focus Area (Epic)                               | Status |
|---------|-------------------------------------------------|--------|
| **v0.1.0** | Minimal static chatbot (single prompt)         | âœ… Done |
| **v0.2.0 â€“ 0.2.5** | Multi-turn memory, specialised prompts, UI polish | âœ… Done |
| **v0.3.0** | Structured experiments & diagnostics panel     | âœ… Done |
| **v0.4.0** | Alias-driven prompt matching                   | âœ… Done |
| **v0.4.1** | Safety guard-rails & post-filtering            | âœ… Done |
| **v0.4.2** | Context trimming & debug logging               | âœ… Done |
| **v0.4.3** | *Current* â€“ In-memory backend + summarise scaffold | ðŸ”„ In Progress |
| **v0.4.4** | **Persistent memory (Redis/SQLite) + Summarise-MVP** | ðŸ”œ Planned |
| **v0.4.5** | Evaluation harness & expanded guard-rails      | ðŸ”œ Planned |
| **v0.5.0** | Containerisation & CI pipeline (Podman / OpenShift) | ðŸ”œ Planned |
| **v0.6.x** | RAG prototype (file-based Q&A)                 | ðŸ”œ Planned |
| **v0.7.x** | Fine-tuning foundation setup                   | ðŸ”œ Planned |

*(milestone titles match what will be created in GitHub â†’ feel free to rename)*

---

## Learning Outcomes Along the Way

* Prompt design & token-budget management
* Transformer architecture basics (encoder/decoder, LoRA adapters)
* Practical debugging of model behaviour & safety filters
* Building memory layers and context summarisation
* Experiment tracking, CI, and deployment practices
* Serving LLMs as modular, testable services

---

## Related Docs

* [`README.md`](../README.md) â€” high-level project overview
* [`docs/release_notes.md`](./release_notes.md) â€” full changelog
* [`docs/experiments_tracker.md`](./experiments_tracker.md) â€” test logs per version
* [`docs/scope.md`](./scope.md) â€” feature scope & cut-lines
* [`docs/contributing.md`](./contributing.md) â€” workflow & branch naming

> *Remember: this document is expected to evolve every cycle.
> Keep the **Version table** and **Learning Phases** in sync with the Project board for zero-surprises planning.*
