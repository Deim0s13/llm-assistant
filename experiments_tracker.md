# ðŸ§ª LLM Experiments Tracker

This file tracks all experiments conducted as part of the `feature/v0.3.0-llm-experiments` branch.

| Experiment ID | Title                         | Status  | Notes                                                         |
|:--------------|:------------------------------|:--------|:--------------------------------------------------------------|
| 1             | Prompt Engineering Basics     | Planned | Explore how different phrasings change model responses        |
| 2             | Context Window Size Effects   | Planned | Study how much previous history influences output             |
| 3             | Generation Parameter Tuning   | Planned | Test temperature, top-p, and max_new_tokens                   |
| 4             | Response Chaining (Self-Loop)  | Planned | Pass output back into the model for follow-up conversations   |
| 5             | Safety and Bias Exploration   | Planned | Deliberately test for hallucinations, bias, and safety issues |

---

## Progress Status

- **Planned** = Experiment idea written but not yet started
- **Active** = Currently being worked on
- **Done** = Completed with observations/notes

---

## Experiment Log

### Experiment ID: [Insert ID]

**Title:** [Insert Title]  
**Status:** Planned / Active / Done  
**Date:** [Insert Date]

---

## Objective

_What are you aiming to learn, validate, or explore in this experiment?_

---

## Setup

- **Model:** `google/flan-t5-base`
- **Prompt Example:** [Insert Prompt Example]
- **Parameters:**
  - Max New Tokens: [Insert Value]
  - Temperature: [Insert Value]
  - Top-p: [Insert Value]
  - Do Sample: [True/False]
- **Other Settings:** [Anything else worth noting]

---

## Observations

_List notable behaviours, surprises, errors, strong responses, etc._

---

## Insights

_Summarise what you learnt about LLM behaviour based on this experiment._

---

## Next Actions (Optional)

---