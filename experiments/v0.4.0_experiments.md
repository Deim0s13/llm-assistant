# v0.4.0 Test & Experiment Tracker

This document captures and organises the experiments and tests for version `v0.4.0`, focusing on the updated prompt matching logic, diagnostics, and alias handling improvements.

---

## Version Summary

Version `v0.4.0` introduces significant enhancements to alias matching, including:

- Token-level alias scanning
- Improved diagnostics (alias tracking and fallback reasoning)
- Multi-token alias support
- Refactored structure for maintainability

These changes require structured validation to confirm improvements over prior versions and catch regressions.

---

## Testing Approach

Each test includes:

- **Input**: The user query.
- **Matched Concept**: What alias/concept was resolved.
- **Resolved Prompt**: The prompt used for generation.
- **Output**: The model’s response.
- **Observations**: Notes on correctness, match quality, hallucinations, or safety concerns.

---

## Matched Prompt Tests

### Test 1

**Input:**  
`Explain black holes like I’m five`

**Matched Concept:**  
`explain_simple` (Confidence: N/A)

**Resolved Prompt:**  
_Explain the topic in a very simple way, as if you are talking to a five-year-old._

**Output:**  
> Black holes are a kind of astrophysics.

**Observations:**  
Matched correctly, but the output was vague and not actually “kid-friendly.” Model needs more guidance in handling tone and depth when using this prompt.

---

### Test 2

**Input:**  
`Summarise this article`

**Matched Concept:**  
`summarise` (Confidence: N/A)

**Resolved Prompt:**  
_Summarise this clearly and concisely._

**Output:**  
> The 'Status' is a short list of the most important features of the 'Status'.

**Observations:**  
Resolved correctly, but content lacked coherence and context. Indicates fallback to general language when no actual article is provided. Not necessarily a model or prompt issue.

---

### Test 3

**Input:**  
`Tell me a joke`

**Matched Concept:**  
`funny_response` (Confidence: N/A)

**Resolved Prompt:**  
_Give a light-hearted or humorous reply to this request._

**Output:**  
> I'm a nerd and I'm a nerd.

**Observations:**  
Correct match, but humor fell flat. May need a more tailored prompt or fine-tuned examples in the future to increase reliability for humorous responses.

---

### Test 4

**Input:**  
`What’s the difference between RAM and SSD?`

**Matched Concept:**  
`compare_items` (Confidence: N/A)

**Resolved Prompt:**  
_Compare the following two items, highlighting their similarities and differences._

**Output:**  
> The difference between the two items is that the memory is a different type of memory, and the SSD is a different type of memory.

**Observations:**  
Correct prompt used, but output was nonsensical. Indicates that prompt context alone may be insufficient—might need embedded keyword injection (e.g., `RAM vs SSD`) in prompt structure or fallback context support.

---

### Test 5

**Input:**  
`Who painted the Mona Lisa?`

**Matched Concept:**  
`Base Prompt Used (no match found)` (Confidence: N/A)

**Resolved Prompt:**  
_(Standard assistant prompt with examples)_

**Output:**  
> Leonardo da Vinci.

**Observations:**  
Performed correctly using base prompt. No issues, demonstrates that fallback still yields good results for simple factual queries.

---

## Summary of Test Results

| Test | Matched Prompt       | Output Quality      | Observations                                      |
|------|----------------------|---------------------|---------------------------------------------------|
| 1    | `explain_simple`     | Vague             | Concept matched but lacked actual simplification |
| 2    | `summarise`          | Weak context      | Expected—no article was provided                 |
| 3    | `funny_response`     | Flat humour       | Needs better tuning or example-based guidance    |
| 4    | `compare_items`      | Incoherent        | Prompt structure correct, but content poor       |
| 5    | `base_prompt`        | Accurate          | Worked as expected with fallback base prompt     |

---

## Structure

All experiments are stored under the `/experiments` folder, with one document per version.
This tracker acts as the master index linking to each version-specific log.

### Past Experiments

- [v0.2.4 Experiments](./experiments/v0.2.4.md)
- [v0.3.0 Experiments](./experiments/v0.3.0.md)

### Current

- v0.4.0 — **In Progress**

---

## Status

This file serves as both a checklist and central summary of test results for the release. Once testing concludes, results will be included inline or linked to a dedicated markdown log.