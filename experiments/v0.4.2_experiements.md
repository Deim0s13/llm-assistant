# v0.4.2 Test & Experiment Tracker

This document captures structured tests for version `v0.4.2`, which focuses on:
- Context memory windowing and trimming logic
- Project hygiene and GitHub Projects workflow adoption
- Cross-platform consistency across macOS and Windows

---

## Test 1 â€” Short History (Under Limit)

**Test File:**
`experiments/test_history_short.py`

**Context Settings:**
- `max_history_turns`: 5
- `max_prompt_tokens`: 512
- `DEBUG_MODE`: True

**History Input:**
```json
[
  { "role": "user", "content": "Hello" },
  { "role": "assistant", "content": "Hi! How can I help?" },
  { "role": "user", "content": "What's the weather like today?" }
]
```

**Lastest Message:**

---

### Expected Outcome

- All 3 messages retained (no trimming needed)
- Final prompt includes base prompt + history + new message
- `source = base_prompt`
- Context logs appear (message count, token count)

---

### Actual Output

- âœ… Base prompt appeared correctly
- âœ… All messages present in context
- âœ… Final user message injected as expected
- âœ… `[Context Debug]` logging confirmed in console
- âœ… `source` identified as `base_prompt`

---

### Observations

- Trimming logic was bypassed since input length < `max_history_turns`
- Prompt structure remained intact and clean
- Good baseline to compare future trimming tests

---

### Verdict

**PASS**
Correct context handling confirmed when history is shorter than the configured memory window.

---

## Test 2 â€” Trimmed History (Over Limit)

**Test File:**
`experiments/test_history_trim.py`

**Context Settings:**
- `max_history_turns`: 5
- `max_prompt_tokens`: 512
- `DEBUG_MODE`: True

**History Input:**
8 total messages (4 user, 4 assistant)

**Latest Message:**
`Tell me how frogs breathe.`

---

### Expected Outcome

- Only the last 5 turns are retained
- Prompt includes truncated history + new message
- `[Context Debug]` logs should show count before and after trimming
- `source = base_prompt`

---

### Actual Output

- âœ… Base prompt loaded correctly
- âœ… 5 most recent turns retained (trimmed first 3)
- âœ… Final user message appears cleanly
- âœ… Prompt formatting intact and readable
- âœ… `source = base_prompt`

---

### Observations

- Trimming occurred as expected
- Prompt continued to flow naturally with no structure issues
- Logs (if visible) confirm correct debug output

---

### Verdict

**PASS**
Context trimming behaves correctly when history exceeds the configured limit.

---

## Test 3 â€” Exact History (Edge Case)

**Test File:**
`experiments/test_history_exact.py`

**Context Settings:**
- `max_history_turns`: 5
- `max_prompt_tokens`: 512
- `DEBUG_MODE`: True

**History Input:**
5 messages exactly (mix of user and assistant)

**Latest Message:**
`How do frogs breathe?`

---

### âœ… Expected Outcome

- All 5 turns are retained
- No trimming occurs
- Prompt structure is clean and complete
- `source = base_prompt`

---

### ðŸ§ª Actual Output

- âœ… Base prompt present
- âœ… History appears in full with 5 retained turns
- âœ… Final user message added successfully
- âœ… Prompt structure was correctly formatted
- âœ… `source = base_prompt`

---

### ðŸ“ Observations

- Behaviour at the edge of the history limit was consistent
- No issues with token formatting or message order
- Logging confirmed the correct number of messages retained

---

### âœ… Verdict

**PASS**
System handled the `max_history_turns` boundary condition as expected.
